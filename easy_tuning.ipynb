{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoModel\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "\n",
    "repo_id = \"Qwen/Qwen3-0.6B-Base\"\n",
    "#repo_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map='cuda:0', \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             torch_dtype='auto')\n",
    "print(f'Model memory footprint: {model.get_memory_footprint()/1e6} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model memory footprint: {model.get_memory_footprint()/1e9} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5817ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "layer_params = defaultdict(int)\n",
    "# Accumulate parameters per layer (by prefix)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        # Extract the layer/module name (e.g., \"transformer.h.0.attn\")\n",
    "        layer_name = \".\".join(name.split(\".\")[:3])  # Adjust depth as needed\n",
    "        layer_params[layer_name] += param.numel()\n",
    "\n",
    "# Print parameter count per layer\n",
    "for layer_name, param_count in sorted(layer_params.items()):\n",
    "    print(f\"{layer_name:<60} {param_count:,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(r = 8,\n",
    "                    lora_alpha=16,\n",
    "                    bias='none',\n",
    "                    lora_dropout=0.05,\n",
    "                    task_type='CAUSAL_LM',\n",
    "                    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "#model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727390e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"voidful/reasoning_gemini_300k\", num_proc=8)\n",
    "ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(example):\n",
    "    converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": example['message']},\n",
    "            {\"role\": \"assistant\", \"content\": '<think>' + example['reasoning'] + '</think>' + example['answer']},\n",
    "        ]\n",
    "    return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(format_dataset, num_proc=8)\n",
    "ds = ds.remove_columns(['message', 'reasoning', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(ds[0]['messages'], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=8,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=2, \n",
    "    max_length=1024,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=False,\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.2,\n",
    "\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='paged_adamw_8bit',  \n",
    "    max_steps=500,     \n",
    "\n",
    "    dataloader_num_workers=8,\n",
    "    dataset_num_proc=8,\n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./qwen3_adapter',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f24801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
