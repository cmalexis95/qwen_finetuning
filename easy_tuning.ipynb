{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoModel\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b00458",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_8bit=True\n",
    ")\n",
    "\n",
    "repo_id = \"Qwen/Qwen3-8B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map='cuda:0', \n",
    "                                             quantization_config=bnb_config,\n",
    "                                             use_cache=False,\n",
    "                                             torch_dtype='auto')\n",
    "print(f'Model memory footprint: {model.get_memory_footprint()/1e9} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "peft_config = LoraConfig(r = 8,\n",
    "                         lora_alpha=16,\n",
    "                         bias='none',\n",
    "                         lora_dropout=0,\n",
    "                         task_type='CAUSAL_LM',\n",
    "                         target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                         \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def format_dataset(example):\n",
    "    instruction = example['instruction']\n",
    "    output = example['output']\n",
    "\n",
    "    # Remove ser blocks\n",
    "    output = re.sub(r'<ser>.*?</ser>', '', output, flags=re.DOTALL)\n",
    "    \n",
    "    # Use a regular expression to match the entire block starting with <think> and ending with the explanation after </think>\n",
    "    think_blocks = re.findall(r'(<think>.*?</think>.*?)(?=\\n<think>|$)', output, re.DOTALL)\n",
    "    \n",
    "    # Create a list of messages\n",
    "    converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "        ]\n",
    "    \n",
    "    for block in think_blocks:\n",
    "        converted_sample.append({\"role\": \"assistant\", \"content\": block})\n",
    "\n",
    "    return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HelpingAI/Intermediate-Thinking-130k\", split='train')\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns(['instruction','input','output','conversation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bcbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(dataset[0]['messages'], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8354dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=32,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=4, \n",
    "    max_length=1024,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=False,\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=0.2,\n",
    "\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    #optim='adamw_8bit',  \n",
    "    max_steps=100,     \n",
    "\n",
    "    dataloader_num_workers=8,\n",
    "    dataset_num_proc=8,\n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=1,\n",
    "    log_level='info',\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./qwen3_adapter',\n",
    "    report_to='none',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f24801",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fe56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f3033-7cc2-450d-b3b3-e5a7750712ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('qwen3_adapter/checkpoint-500/trainer_state.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "loss, lr = [], []\n",
    "for step in data['log_history']:\n",
    "    loss.append(step['loss'])\n",
    "    lr.append(step['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fadcec-6751-4031-a1f2-b7658632008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2a4bcb-3d92-46af-bbba-8487da7e94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lr)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c28d683-112f-40ef-a644-8536421a86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "repo_id = \"Qwen/Qwen3-0.6B-Base\"\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map='cuda:0',\n",
    "                                             torch_dtype='auto')\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    model, 'qwen3_adapter/checkpoint-500/', torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090585b0-e89f-49c0-af70-06c9366fe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731572f-86bd-41c9-91b5-da4585ab27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915ca14-8778-42f5-a1f6-0e84705e7617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4846ce0-0aee-4123-8089-7963836dcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a566fb-3018-45ce-a681-0feaca14d809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
