{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2a209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e53f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(repo_id,\n",
    "                                             device_map='cuda:0',\n",
    "                                             torch_dtype='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ecad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model memory footprint: {model.get_memory_footprint()/1e9} GB')\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params/1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c04d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9437af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"voidful/reasoning_gemini_300k\", num_proc=8)\n",
    "ds = ds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86025e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Top 10 attractions in paris\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(outputs.logits[0], inputs[0])\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c610f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: grad type = {param.grad.dtype}\")\n",
    "    else:\n",
    "        print(f\"{name}: No gradient computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c341709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(example):\n",
    "    converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": example['message']},\n",
    "            {\"role\": \"assistant\", \"content\": '<think>' + example['reasoning'] + '</think>' + example['answer']},\n",
    "        ]\n",
    "    return {'messages': converted_sample}\n",
    "ds = ds.map(format_dataset)\n",
    "ds = ds.remove_columns(['message', 'reasoning', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc385bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=False,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=2,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=1, \n",
    "    max_length = 256,\n",
    "    max_steps=50,\n",
    "    bf16=True,\n",
    "    \n",
    "    ## GROUP 2: Dataset-related\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=False,\n",
    "    dataset_num_proc=8,\n",
    "    dataloader_num_workers=8,\n",
    "    include_tokens_per_second=True,\n",
    "    include_num_input_tokens_seen=True,\n",
    "    \n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    # Optimizer\n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=1,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./qwen3-adapter',\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e40564",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08757c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dtypes of optimizer states\n",
    "for i, (key, state) in enumerate(trainer.optimizer.state.items()):\n",
    "    print(f\"\\nParameter {i}:\")\n",
    "    for state_key, value in state.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            print(f\"  {state_key}: dtype = {value.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef451d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (param, state) in enumerate(trainer.optimizer.state.items()):\n",
    "    print(f\"\\nParameter {i}:\")\n",
    "    param_size = param.numel() * param.element_size()\n",
    "    print(f\"  - Model parameter: {param.numel()} elements * {param.element_size()} bytes = {param_size} bytes\")\n",
    "\n",
    "    for key, value in state.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            state_size = value.numel() * value.element_size()\n",
    "            print(f\"  - {key}: {value.numel()} elements * {value.element_size()} bytes = {state_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9b477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
